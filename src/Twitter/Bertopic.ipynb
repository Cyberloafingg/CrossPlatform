{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "\n",
    "import langid\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 读取文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TWITTER_POST_PATH = \"./data/2024_0131/Twitter/meibo.json\"\n",
    "post_all_data = []\n",
    "with open(TWITTER_POST_PATH, 'r',encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "    for line in tqdm(lines):\n",
    "        data = json.loads(line)\n",
    "        post_all_data.append(data)\n",
    "print(len(post_all_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取lang_distribution.json\n",
    "import json\n",
    "with open('./data/2024_0131/Twitter/lang_distribution.json', 'r',encoding='utf-8') as f:\n",
    "    lang_dict = json.load(f)\n",
    "# 统计每个语言的数量\n",
    "lang_count = {}\n",
    "for key in lang_dict:\n",
    "    lang_count[key] = len(lang_dict[key])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 处理英文语料\n",
    "1. 目前的预处理考虑，去除换行符、将网址转换为空字符串、去掉@的人？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_data = lang_dict['en']\n",
    "print(len(en_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_newlines(text):\n",
    "    # 将换行符替换为空格\n",
    "    clean_text = text.replace('\\n', ' ')\n",
    "    return clean_text\n",
    "def remove_urls(text):\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    # 将匹配到的网址替换为空字符串\n",
    "    clean_text = url_pattern.sub('', text)\n",
    "    return clean_text\n",
    "def remove_after_at(text):\n",
    "    # 匹配@符号后面的单词的正则表达式\n",
    "    after_at_pattern = re.compile(r'@\\w+\\s?')\n",
    "    clean_text = after_at_pattern.sub('', text)\n",
    "    return clean_text\n",
    "def remove_punctuation(text):\n",
    "    clean_text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return clean_text\n",
    "def convert_to_lowercase(text):\n",
    "    return text.lower()\n",
    "english_stopwords = [\n",
    "    'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself',\n",
    "    'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself',\n",
    "    'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these',\n",
    "    'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do',\n",
    "    'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while',\n",
    "    'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before',\n",
    "    'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again',\n",
    "    'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each',\n",
    "    'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than',\n",
    "    'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now','RT'\n",
    "]\n",
    "def remove_stopwords(text, stopwords):\n",
    "    words = text.split()  # 将文本分词为单词列表\n",
    "    clean_words = [word for word in words if word not in stopwords]  # 去除停用词\n",
    "    clean_text = ' '.join(clean_words)  # 将列表中的单词重新组合成文本\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "for data in tqdm(en_data):\n",
    "    original_text = remove_newlines(data['text'])\n",
    "    remove_url = remove_urls(original_text)\n",
    "    remove_after = remove_after_at(remove_url)\n",
    "    remove_punctuation_text = remove_punctuation(remove_after)\n",
    "    convert_lowercase = convert_to_lowercase(remove_punctuation_text)\n",
    "    remove_stopword = remove_stopwords(convert_lowercase, english_stopwords)\n",
    "    en_data[idx]['text'] = remove_stopword\n",
    "    # print(f\"original text: {original_text}\")\n",
    "    # print(f\"remove url: {remove_url}\")\n",
    "    # print(f\"remove after at: {remove_after}\")\n",
    "    # print(f\"remove punctuation: {remove_punctuation_text}\")\n",
    "    # print(f\"convert lowercase: {convert_lowercase}\")\n",
    "    # print(f\"remove stopword: {remove_stopword}\")\n",
    "    # print()\n",
    "    # idx += 1\n",
    "    # if idx == 5:\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_query_dict = {}\n",
    "for data in en_data:\n",
    "    query = data['query']\n",
    "    if query in en_query_dict:\n",
    "        en_query_dict[query] += 1\n",
    "    else:\n",
    "        en_query_dict[query] = 1\n",
    "en_query_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_topic = []\n",
    "\n",
    "for query_dict_key in en_query_dict:\n",
    "    detect_lang = langid.classify(query_dict_key)\n",
    "    if detect_lang[0] == 'en' or detect_lang[0] == 'es':\n",
    "        if query_dict_key not in en_topic:\n",
    "            en_topic.append(query_dict_key)\n",
    "print(f\"en_topic: {len(en_topic)}\")\n",
    "en_topic_dict = {\n",
    "    'other': 0,\n",
    "    'U.S. Presidential Election': 0\n",
    "}\n",
    "for data in en_data:\n",
    "    if data['query'] in en_topic:\n",
    "        if data['query'] in en_topic_dict:\n",
    "            en_topic_dict[data['query']] += 1\n",
    "        else:\n",
    "            en_topic_dict[data['query']] = 1\n",
    "    else:\n",
    "        en_topic_dict['other'] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "en_topic_count = Counter(en_topic_dict)\n",
    "en_topic_count = dict(en_topic_count)\n",
    "en_topic_count = dict(sorted(en_topic_count.items(), key=lambda item: item[1], reverse=True))\n",
    "plt.bar(en_topic_count.keys(), en_topic_count.values(),color = 'skyblue')\n",
    "plt.xticks(rotation=90)\n",
    "# 显示每个柱状图的数值\n",
    "for x, y in en_topic_count.items():\n",
    "    plt.text(x, y + 5, '%d' % y, ha='center', va='bottom',rotation=90,fontsize=7)\n",
    "plt.title('en topic count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_names = list(en_topic_count.keys())\n",
    "print(f\"category_names: {len(category_names)}\")\n",
    "# 去除other\n",
    "category_names.pop(category_names.index('other'))\n",
    "print(f\"category_names: {len(category_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = []\n",
    "for data in en_data:\n",
    "    if data['query'] in category_names:\n",
    "        categories.append(category_names.index(data['query']))\n",
    "    else:\n",
    "        categories.append(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "docs = [data['text'] for data in en_data]\n",
    "topic_model = BERTopic(verbose=True,language=\"english\").fit(docs, y=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.save(\"225_semi_en_model\",serialization=\"pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.reduce_topics(docs, nr_topics=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [category_names[i] for i in categories]\n",
    "topics_per_class = topic_model.topics_per_class(docs, classes=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_topics_per_class(topics_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchical_topics = topic_model.hierarchical_topics(docs)\n",
    "topic_model.visualize_hierarchy(hierarchical_topics=hierarchical_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "from sentence_transformers import SentenceTransformer\n",
    "sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embeddings = sentence_model.encode(docs, show_progress_bar=True)\n",
    "\n",
    "topic_model.visualize_hierarchical_documents(docs, hierarchical_topics, embeddings=embeddings)\n",
    "# Reduce dimensionality of embeddings, this step is optional but much faster to perform iteratively:\n",
    "reduced_embeddings = UMAP(n_neighbors=10, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings)\n",
    "topic_model.visualize_hierarchical_documents(docs, hierarchical_topics, reduced_embeddings=reduced_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_document_info(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "topic_model = BERTopic.load(\"225_semi_en_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [data['text'] for data in en_data]\n",
    "topic_model.reduce_topics(docs, nr_topics=100)\n",
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd_topic = pd.DataFrame(topic_model.get_topic_info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_topic.iloc[20:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CP",
   "language": "python",
   "name": "cp"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
